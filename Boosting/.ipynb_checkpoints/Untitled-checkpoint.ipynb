{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost算法\n",
    "\n",
    "生成单个结点：树桩\n",
    "1. 遍历所有的特征\n",
    "2. 以一定步长（或者自己生成一些切分点，比如相邻数据的中点）便利所有的切分点，得到误差$e_m=\\sum_{i=1}^{N}w_{mi}I(G(x)\\neq y_i)$\n",
    "3. 找到最小的使$e_m$最小的切分点和特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "give specific split point and get the stump label\n",
    "parameter:\n",
    "    dataset: dataset\n",
    "    index: index\n",
    "    value: split point value\n",
    "    label: all labbel\n",
    "return:\n",
    "    label: split label\n",
    "'''\n",
    "def subset_label(dataset, index, value, ope):\n",
    "    ret_label = np.ones(dataset.shape[0])\n",
    "    \n",
    "    if ope == 'lt':\n",
    "        ret_label[dataset[:, index] < value] = -1\n",
    "    else:\n",
    "        ret_label[dataset[:, index] >= value] = -1\n",
    "    \n",
    "    return ret_label\n",
    "\n",
    "'''\n",
    "stump generation\n",
    "parameter:\n",
    "    dataset: data\n",
    "    labels: labels\n",
    "    weight: weight D\n",
    "'''\n",
    "def generate_stump(dataset, labels, weight):\n",
    "    n, m = dataset.shape\n",
    "    \n",
    "    best_em = np.inf\n",
    "    best_index = 0\n",
    "    best_split_point = [0, 'lt']\n",
    "    max_num = 10; \n",
    "    for i in range(m):\n",
    "        min_value = dataset[:, i].min()\n",
    "        max_value = dataset[:, i].max()\n",
    "        stride =  (max_value - min_value) / max_num\n",
    "        for ope in ['lt', 'gt']:\n",
    "            for j in range(-1, max_num+1):\n",
    "                split_point = min_value + stride*j\n",
    "                sub_labels = subset_label(dataset, i, split_point, ope)\n",
    "                em = np.sum(weight[sub_labels != labels])\n",
    "                # print(\"split: {} {} Em {}\".format(ope, split_point, em))\n",
    "                \n",
    "                if em < best_em:\n",
    "                    best_em = em\n",
    "                    best_index = i\n",
    "                    best_split_point = [split_point, ope]\n",
    "    return best_em, best_index, best_split_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test function\n",
    "\n",
    "1. 加载简单的数据\n",
    "2. 测试是否可以找到最好的切分点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load_dataset, generate the dataset and labels\n",
    "'''\n",
    "def load_dataset():\n",
    "    data = np.array([0,1,2,3,4,5,6,7,8,9]).reshape(-1, 1)\n",
    "    labels = np.array([1,1,1,-1,-1,-1,1,1,1,-1])\n",
    "    \n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004 0 [2.7, 'gt']\n"
     ]
    }
   ],
   "source": [
    "dataset, labels = load_dataset()\n",
    "D0 = np.ones_like(labels)/len(labels)\n",
    "\n",
    "best_em, best_index, best_split = generate_stump(dataset, labels, D0)\n",
    "print(best_em, best_index, best_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complete process\n",
    "\n",
    "1. 循环的进行子分类器的训练，得到一系列的子分类器\n",
    "2. 根据em，获取每个子分类器的权重\n",
    "3. 根据分类误差，计算下一个子分类器的weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "update weight\n",
    "parameter:\n",
    "    dataset: dataset\n",
    "    label: labels\n",
    "    weight: last weight\n",
    "    alpha: sub-classifier's weight\n",
    "    split_point: split point\n",
    "'''\n",
    "def update_weight(dataset, labels, weightk, alphak, Gx):\n",
    "    idx, value, ope = Gx\n",
    "    \n",
    "    res_labels = np.ones_like(labels)\n",
    "    if ope == 'lt':\n",
    "        res_labels[dataset[:, idx] < value] = -1\n",
    "    else:\n",
    "        res_labels[dataset[:, idx] >= value] = -1\n",
    "    '''\n",
    "    update weight\n",
    "    '''\n",
    "    weight = weightk*np.exp(-alphak*labels*res_labels)\n",
    "    weight = weight/weight.sum()\n",
    "    \n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pred_result\n",
    "parameter:\n",
    "    dataset: dataset\n",
    "    idx: feature index\n",
    "    value: model parameter\n",
    "    ope: operator\n",
    "'''\n",
    "def pred_result(dataset, idx, value, ope):\n",
    "    N = dataset.shape[0]\n",
    "    res = np.ones(N)\n",
    "    \n",
    "    if ope == 'lt':\n",
    "        res[dataset[:, idx] < value] = -1\n",
    "    else:\n",
    "        res[dataset[:, idx] >= value] = -1\n",
    "        \n",
    "    return res\n",
    "\n",
    "'''\n",
    "pred: predict the result\n",
    "parameter:\n",
    "    dataset: dataset\n",
    "    idx: feature index\n",
    "    value: model parameter\n",
    "    ope: operator\n",
    "'''\n",
    "def pred(dataset, labels, model):\n",
    "    add = np.zeros_like(labels)\n",
    "    for idx, value, ope, alpha in model:\n",
    "        res = pred_result(dataset, idx, value, ope)\n",
    "        add = add + alpha*res\n",
    "    \n",
    "    add[add >= 0] = 1\n",
    "    add[add < 0] = -1\n",
    "    add = add.astype(labels.dtype)\n",
    "    return np.sum(add != labels)\n",
    "    \n",
    "'''\n",
    "train adaboost\n",
    "parameter:\n",
    "    dataset: dataset\n",
    "    labels: label\n",
    "'''\n",
    "def train(dataset, labels, M, toler):\n",
    "    n, m = dataset.shape\n",
    "    last_weight = np.ones_like(labels)/n\n",
    "    \n",
    "    models = []\n",
    "    for i in range(M):\n",
    "        ''' generate stump '''\n",
    "        Em, idx, split = generate_stump(dataset, labels, last_weight)\n",
    "        value, ope = split\n",
    "        ''' update alpha '''\n",
    "        alpha = np.log((1-Em)/Em)/2.0\n",
    "        ''' update weight '''\n",
    "        last_weight = update_weight(dataset, labels, last_weight, alpha, [idx, value, ope])\n",
    "        models.append([idx, value, ope, alpha])\n",
    "        error_cnt = pred(dataset, labels, models)\n",
    "        print(\">>> alpha {} Em {} ErrorCnt {}\".format(alpha, Em, error_cnt))\n",
    "        if error_cnt < toler:\n",
    "            break\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> alpha 0.4236489301936017 Em 0.30000000000000004 ErrorCnt 3\n",
      ">>> alpha 0.6496414920651304 Em 0.21428571428571427 ErrorCnt 3\n",
      ">>> alpha 0.752038698388137 Em 0.18181818181818185 ErrorCnt 0\n",
      "[0, 2.7, 'gt', 0.4236489301936017]\n",
      "[0, 8.1, 'gt', 0.6496414920651304]\n",
      "[0, 5.4, 'lt', 0.752038698388137]\n"
     ]
    }
   ],
   "source": [
    "models = train(dataset, labels, 4, 1)\n",
    "for m in models:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
