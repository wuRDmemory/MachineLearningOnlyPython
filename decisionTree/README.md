# 决策树

## 特点
决策树是一种基本的分类与回归方法，是一种树形结构，由内部结点和叶节点组成，其中：
- 内部结点表示一个特征和属性；
- 叶节点表示最终的分类了；

一个实例如下，其中椭圆为内部结点，方框为叶节点：

<img src="imgs/2019-06-27-22-41-22.png" width=500>

&nbsp;
## 决策树与概率分布
决策树表示在给定特征条件下类的条件概率分布，这一条件概率分布定义在对特征空间的划分上。从模型的类型上讲，决策树属于判别模型，即最终求解的是$P(Y|X)$。

&nbsp;
## 决策树的学习
决策树的整个学习过程可以分为：
- 特征选择：局部的学习特征与类别的关系
- 决策树生成：根据选择的特征对数据集进行分类，递归的构建，直到划分之后的特征基本只有一类或者没有合适的特征进行分类
- 决策树剪枝：因为决策树对于特征的过细分类，导致决策树很容易过拟合，解决该问题的方法就是剪枝，将模型变得稍微简单一些，该部分的本质是通过操作使得损失函数（通常是正则化的极大似然函数）最小化。

&nbsp;
## 先验知识
### 熵
熵表示信息的混乱程度（不确定性），通常用H(D)表示，其中X={(x1,y1), (x2,y2)...(xn,yn)}表示数据集，如果不加特殊的说明，H(D)表示对标签y的概率分布P(y)计算熵。
熵的公式如下：
$$
H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
其中如果log的底为2，则单位为比特，若log的底为e，则单位为纳特。
其中$-\log p_{i}$表示信息量（这里可以看做频率的倒数的对数值，P越大值越小），所以熵又可以看做是信息量的期望值。

举个例子来说：假设天气预报告诉你，明天90%的可能性会下雨，那么**该事件的确定性其实很大**，基本上你明天一定会带上雨伞，同理如果天气预报说下雨的可能性只有10%，那么你明天基本上就不会带伞；但是如果天气预报告诉你，明天50%的可能性会下雨，那么此时我们是最纠结的时候，因为**这时候事件的确定性是很小的**，我们最不确定明天是否会下雨。

以上就是一个二值的例子，如果用熵的公式表示这个时间的不确定性的话，可以得到下图：
<img src="imgs/2019-06-28-13-29-35.png" width=500>

可见当p=0.5的时候，事件的混乱度（不确定性）是最大的。

### 条件熵
将上述公式中换做：
$$
H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)
$$
就可以表示在已知其中一个条件$X=x_i$时，事件的不确定性。由公式可以看出，条件熵是条件概率的熵的期望。

例如明天的天气情况X与后天是否有雨Y，我们根据历来的一些数据知道了$P(Y|X={所有今天的天气状况})$ 的概率分布，那么我们就可以得到条件概率的熵，之后对明天的天气情况的概率分布$P(X)$求期望就得到了条件熵。

### 基尼系数
基尼系数与熵类似，也是衡量一个事件确定程度的数值，具体的公式如下：
$$
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
$$

与熵类似，基尼系数对于条件概率表示的事件也能评价其确定性，公式如下：
$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
$$

### 总结
熵和基尼系数其实都是对于数据集确定性的一种度量方式，这里不进行过多介绍。

&nbsp;
## 特征选择
